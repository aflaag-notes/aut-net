\documentclass{beamer}

% Theme

\usepackage{amsfonts,amsmath,oldgerm}
\usefonttheme[onlymath]{serif}
\usetheme{sintef}

% Custom imports

\usepackage{amsfonts,amsmath,oldgerm}
\usepackage[Algorithm]{algorithm}
\usepackage{algpseudocode}
\usepackage[
	backend=bibtex,
	style=alphabetic,
	sorting=anyt,
	minnames=3,
	minalphanames=3
]{biblatex}

% Custom macros

\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\rbk}[1]{\left(#1\right)}
\newcommand{\sbk}[1]{\left[#1\right]}
\newcommand{\cbk}[1]{\left\{#1\right\}}
\newcommand{\curlyquotes}[1]{\textquotedblleft #1\textquotedblright}

% Intro slide

\titlebackground*{assets/background}

\title{Reinforcement Learning Based Routing in Networks: Review and Classification of Approaches}
\subtitle{A comprehensive review of the literature of RL-based protocols}
\course{Master's Degree in Computer Science}
\author{\href{alessio.bandiera02@gmail.com}{Alessio Bandiera}}
\IDnumber{1985878}
\date{}

% Bibliography

\addbibresource{../src/references.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Document

\begin{document}
\maketitle

\section{Introduction}

\footlinecolor{maincolor}
\begin{frame}{Motivation}
	Modern networks have become far more complex, dynamic and diverse than early manually configured systems. This has made \tit{human} management \underline{insufficient}. \\

	\vspace{0.8cm}

	\pause

	As a result, \tbf{Machine Learning (ML)} is used more and more often to handle tasks such as

	\begin{itemize}
		\item traffic prediction
		\item fault and configuration management
		\item congestion control
	\end{itemize}

	\vspace{0.8cm}

	\pause

	The goal is to \tbf{automatically} learn network conditions in order to improve the user experience, while optimizing network resources. \\
\end{frame}

\begin{frame}{The routing problem}
	In networks, \tbf{routing} is the problem of selecting paths for sending packets from source(s) to destination(s), while

	\begin{itemize}
		\item meeting \tbf{Quality of Service (QoS)} requirements
		\item optimizing network resources
	\end{itemize}

	\vspace{0.8cm}

	\pause

	However, whenever multiple metrics are required, the routing problem becomes \tbf{\textsf{NP}-complete}. \\

	\vspace{0.8cm}

	\pause

	This is the reason why ML is seen as a strategy to revolutionize current \tbf{routing} techniques.
\end{frame}

\begin{frame}{Reinforcement Learning}
	In particular, among all the various ML techniques known today, \tbf{Reinforcement Learning (RL)} stands out in terms of adoption for solving the routing problem. \\

	\vspace{0.8cm}

	\pause

	As already discussed throughout the lectures of this course, \tbf{Reinforcement Learning (RL)} is an ML technique inspired by behavioral psychology that provides system modeling based on \tit{agents} that interect with their \tit{environment}. \\

	\vspace{0.8cm}

	\pause

	RL-based algorithms are particularly useful in the context of routing because it can continuously \tbf{learn and adapt} to changing network conditions, selecting routes that optimize performance based on real-time experience rather than fixed rules.
\end{frame}

\begin{frame}{Q-learning}
	In 1989 \textcite{watkins} proposed the most-widely adopted flavour of RL, called \tbf{Q-learning}. \\

	\vspace{0.8cm}

	\begin{overprint}
		\onslide<1>

		Q-learning is a \tbf{model-free} approach that aims at estimating the action function $Q_{\pi^*}(s, a)$, where $\pi^*$ is the optimal policy

		\onslide<2>

		Q-learning is a \tbf{model-free} approach that aims at estimating the action function $Q_{\pi^*}(s, a)$, where $\pi^*$ is the optimal policy \\

		\vspace{0.8cm}

		Very importantly, his approximation of the action function is \tit{independent of the policy} followed by the agent, making Q-learning applicable in a wide variety of contexts.

		\onslide<3>

		The action-value is updated through the following formula: $$Q_n(s_n, a_n) = (1 - \alpha) \cdot Q_{n - 1}(s_n, a_n) + \alpha \cdot \sbk{R_n + \gamma \cdot \max_{a \in \mathcal A}{Q_{n - 1}(s_{n + 1}, a)}}$$ where $\alpha$ is the \tbf{learning factor}, and $\gamma$ is the \tbf{discount rate}.

		\onslide<4>

		Moreover, this function can be rewritten in its more common \tbf{discrete time $t$} form: $$Q(s_t, a_t) = (1 - \alpha) \cdot Q(s_t, a_t) + \alpha \cdot \sbk{R_{t + 1} + \gamma \cdot \max_{a \in \mathcal A}{Q(s_{t + 1}, a)}}$$

		\onslide<5>

		Most importantly, Watkins showed that Q-learning \tbf{converges} to the optimum action-values with probability $1$, as long as all actions are repeatedly sampled in all states. \\

		\vspace{0.8cm}

		Indeed, this is the reason why Q-learning is the most popular and effective learning technique in the field.
	\end{overprint}
\end{frame}

\begin{frame}{Q-routing}
	In 1993 \textcite{qrouting} proposed a hop-by-hop routing algorithm based on Q-learning, called \textbf{Q-routing}. \\

	\vspace{0.8cm}

	\pause

	After their seminal work, tens of works followed the original idea of using RL to optimize routing, while also considering the evolution of commuinication networks and users requirements. \\

	\vspace{0.8cm}

	\pause
	In fact, most of the existing RL-based routing protocols today are extensions of their original work.
\end{frame}

\begin{frame}{The work in exam}
	The paper that will be discussed today was published in 2019 by \textcite{mammeri}. \\

	\vspace{0.8cm}

	Their work is a review of \tbf{60 papers}, which essentially covers the literature of RL-based routing algorithms completely. \\

	\vspace{0.8cm}

	\pause

	In particular, their work has 2 main objectives:

	\begin{overprint}
		\onslide<1>
		\onslide<2>
		\onslide<3>

		\begin{enumerate}
			\item provide a \tbf{comprehensive presentation} of the main characteristics of RL-based routing protocols
		\end{enumerate}

		\onslide<4>

		\begin{enumerate}
			\item provide a \tbf{comprehensive presentation} of the main characteristics of RL-based routing protocols
			\item provide \tbf{classification criteria} to enable analysis and comparison of existing protocols
		\end{enumerate}
	\end{overprint}
\end{frame}

\begin{frame}{Table of contents}
	This presentation will first provide a general idea of \tbf{Q-routing}, which has been a very influential idea and most of the current literature is based on this RL-based algorithm. \\

	\vspace{0.8cm}

	\pause

	Subsequently, the most important segment of this review will be presented: the \tbf{classification criteria} that the authors defined in order to categorize the papers. \\

	\vspace{0.8cm}

	\pause

	To their knowledge, the authors state that their work is the first in the literature that proposes classification criteria to help comparing all available RL-based routing protocols. \\
\end{frame}

\section{Q-routing}

\begin{frame}{Introduction to Q-routing}
	\tbf{Q-routing} is an RL-based approach to distributed routing in packet-switched networks. \\

	\vspace{0.8cm}

	\pause

	Each node maintains \tbf{estimates} of the delivery time to every destination through each neighbor, and updates these values through \tit{continuous interaction} with the network. \\

	\vspace{0.8cm}

	\pause

	By learning from real traffic rather than relying on static metrics, Q-routing adapts to

	\begin{itemize}
		\item congestion
		\item topology changes
		\item varying link qualities
	\end{itemize}

	allowing routing decisions to improve dynamically over time.
\end{frame}

\begin{frame}{The algorithm}
	\scriptsize
	\begin{algorithmic}[1]
		\Function{Qrouting}{ }
		\State Initialize $Q_i$ matrix randomly
		\While{termination condition holds}
		\If{packet $P$ is ready to be sent to $d$}
		\State Determine node $j^* \gets \argmin_{j \in \mathcal N (i)}{Q_i(d, j)}$
		\State Send packet to node $j^*$
		\State Collect estimate $\theta_{j^*}(d)$ from node $j^*$
		\State Update $Q_i(d, j^*) \gets (1 - \alpha) \cdot Q_i(d, j^*) + \alpha \cdot \sbk{W_i^q(P) + T_{ij^*} + \theta_{j^*}(d)}$
		\EndIf
		\EndWhile
		\EndFunction
	\end{algorithmic}

	\begin{itemize}
		\item $i$ is the node that is currently running the algorithm
		\item $P$ is a packet that node $i$ needs to forward to destination $d$
		\item $Q_i(d, j)$ is the \textit{delivery delay} that $i$ estimates it takes, for node $j$, to deliver the packet $P$ at destination $i$
		\item $\mathcal N(j)$ is the set of $j$'s neighbors
		\item $\theta_j(d)$ is $j$'s estimate for the time remaining in the trip to destination $d$ of packet $P$
		\item $W_i^q(P)$ is the time spent by packet $P$ in node $i$'s queue
		\item $T_{i j}$ is the transmission time between nodes $i$ and $j$
	\end{itemize}
\end{frame}

\begin{frame}{The algorithm}
	\scriptsize
	\begin{algorithmic}[1]
		\Function{Qrouting}{ }
		\State Initialize $Q_i$ matrix randomly
		\While{termination condition holds}
		\If{packet $P$ is ready to be sent to $d$}
		\State Determine node $j^* \gets \argmin_{j \in \mathcal N (i)}{Q_i(d, j)}$
		\State Send packet to node $j^*$
		\State Collect estimate $\theta_{j^*}(d)$ from node $j^*$
		\State Update $Q_i(d, j^*) \gets (1 - \alpha) \cdot Q_i(d, j^*) + \alpha \cdot \sbk{W_i^q(P) + T_{ij^*} + \theta_{j^*}(d)}$
		\EndIf
		\EndWhile
		\EndFunction
	\end{algorithmic}

	\vspace{0.8cm}

	\normalsize

	\begin{overprint}
		\onslide<1>
		Upon sending packet $P$ to node $j^*$, node $i$ receives back from node $j^*$ the estimate $$\theta_{j^*}(d) = \min_{k \in \mathcal N(j^*)}{Q_{j^*}(d, k)}$$

		\onslide<2>
		Then, node $i$ updates $Q_i(d, j^*)$ based on the \tit{update formula} for Q-learning described earlier: $$Q(s_t, a_t) = (1 - \alpha) \cdot Q(s_t, a_t) + \alpha \cdot \sbk{R_{t + 1} + \gamma \cdot \max_{a \in \mathcal A}(s_{t + 1}, a)}$$
	\end{overprint}
\end{frame}

\begin{frame}{Flaws of Q-learning}
	Despite the wide adoption, Q-routing has some flaws. Some problems are direct consequences of Q-learning such as

	\begin{itemize}
		\item \tit{slow convergence}
		\item \tit{high parameter setting sensitivity}
	\end{itemize}

	\vspace{0.8cm}

	\pause

	However, there are also problems arising from the algorithm itself, for instance the \textbf{Q-value freshness}: $\theta_j(d)$ is evaluated only upon packet transmission on a route, therefore if a route is not used for a long time its estimate becomes \textit{outdated}.
\end{frame}

% \begin{frame}{From Q-routing to moden RL-based protocols}
% 	Q-routing introduced the idea that routers can \tit{learn} optimal paths through experience rather than relying on fixed metrics. \\
%
% 	\vspace{0.8cm}
%
% 	\pause
%
%
% 	As anticipated, its core principles laid the foundation for a \tit{wide range} of RL-driven routing algorithms developed in the following years. \\
%
% 	\vspace{0.8cm}
%
% 	\pause
%
% 	Building on this early work, the literature now includes numerous protocols that extend and modify these ideas to address modern networking challenge.
% \end{frame}

\section{Classification criteria}

\begin{frame}{The criteria groups}
	To make sense of this growing body of research, the authors introduced a set of \tbf{classification criteria} that allow to systematically \tit{categorize} and compare RL-based routing approaches of the literature. \\

	\vspace{0.8cm}

	\pause

	These criteria are divided into 3 groups:

	\begin{enumerate}
		\item \tbf{Context of use}: criteria based on the \tit{target applications}
		\item \tbf{Design characteristics}: criteria based on the \tit{design} of the protocols
		\item \tbf{Performance}: criteria based on qualitative evaluation on \tit{overhead} and \tit{metrics}
	\end{enumerate}
\end{frame}

\begin{frame}{The criteria groups}
	TODO add image
\end{frame}

\begin{frame}
	\frametitle{Context of use}
	\framesubtitle{Classification criteria: Context of use}

	The first group of criteria focuses on the \tbf{context} in which an RL-based routing protocol is applied. \\

	\vspace{0.8cm}

	\pause
	This includes the nature of the target application and the operational conditions under which the protocol must perform.
\end{frame}

\begin{frame}
	\frametitle{Addressed network classes}
	\framesubtitle{Classification criteria: Context of use}

	Not suprisingly, the first criteria of categorization is the \tbf{type of network} of the protocol, since there are various different types:

	\begin{itemize}
		\item WSNs
		\item DTNs
		\item FANETs
		\item MANETs
		\item etc.
	\end{itemize}

	\vspace{0.8cm}

	\pause

	Moreover, some protocols rely on \tbf{specific assumptions} such as having

	\begin{itemize}
		\item prior knowledge of traffic distributions
		\item node localization services
		\item the possibility of transmission errors
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Routing optimization context}
	\framesubtitle{Classification criteria: Context of use}

	The ability of a protocol to perform path selection optimally depends on

	\begin{itemize}
		\item the roles assigned to data sources
		\item the roles assigned to relaying nodes
		\item the initial assumptions about routing
	\end{itemize}

	\pause

	From this observation, the authors outlined 6 different \tbf{routing optimization contexts}:

	\begin{overprint}
		\onslide<1>
		\onslide<2>

		\begin{enumerate}
			\item \tbf{Data-packet driven optimization context}:
			      \begin{itemize}
				      \item the transmission of packets happens \tit{hop-by-hop} from $s$ to $d$
				      \item upon receival of a packet, $d$ sends back a feedback on the transmission
			      \end{itemize}

			      After some amount of forwarded packets, the routing process converges to the selection of optimal paths
		\end{enumerate}

		\onslide<3>

		\begin{enumerate}
			\setcounter{enumi}{1}
			\item \tbf{Route request diven optimization context}:

			      \begin{itemize}
				      \item a source $s$ that has data to send to $d$ first sends a \tit{Route Request (RR)} packet, which is disseminated in the network
				      \item each node can decide to partecipate or not: if a node agrees to partecipate, it selects the next node to forward the RR to, and this process continues until $d$ is reached
				      \item once a path is found, all packets $s \to d$ are routed through this path
				      \item at the end of each transmission a feedback is sent back to $s$
			      \end{itemize}

			      Most protocols in this category are extensions to the \tbf{AODV} protocol \cite{aodv}.
		\end{enumerate}

		\onslide<4>

		\begin{enumerate}
			\setcounter{enumi}{2}
			\item \tbf{Context request driven optimization context}: it describes P2P systems

			      \begin{itemize}
				      \item in this context a node $s$ is interested in some content $C$ possessed by $d$
				      \item then, it sends a request to $d$ to receive data packets from $d$
				      \item nodes on the path $s \to d$ can then decide to forward the requrest to locate the requested content
				      \item when data packets containing $C$ are forwarded the relay nodes receive feedback and adapt their paths accordingly
			      \end{itemize}
		\end{enumerate}

		\onslide<5>

		\begin{enumerate}
			\setcounter{enumi}{3}
			\item \tbf{Predefined routes driven optimization context}:

			      \begin{itemize}
				      \item each source builds \tit{offline} a list of paths of reachability for any target destination
				      \item when a source has packets to send it selects a path among the predefined ones
				      \item if a \tit{link break} is detected on the selected path, the source switches to another predefined path
				      \item periodically, a feedback is sent backward to the source, which will adapt its path selection
			      \end{itemize}
		\end{enumerate}

		\onslide<6>

		\begin{enumerate}
			\setcounter{enumi}{4}
			\item \tbf{Cluster driven optimization context}:

			      \begin{itemize}
				      \item a \tit{cluster} is a partition of the nodes that has a \tit{cluster-head}
				      \item data patckets are transmitted from $s$ to $d$ following a clustered hierarchy of the network
				      \item a cluster-head determines the number of members that can join in the cluster depending on the resources
				      \item after a transmission, cluster-heads receive feedback and adjust their cluster size accordingly
			      \end{itemize}
		\end{enumerate}

		\onslide<7>

		\begin{enumerate}
			\setcounter{enumi}{5}
			\item \tbf{Routing protocol driven optimization context}:

			      \begin{itemize}
				      \item a \tit{central node} has a set of routing protocols candidates (e.g. AODV, DSDV, DSR, OLSR, GPSR, etc.) that can be used for forwarding packets to \tit{slave nodes}
				      \item in each $\Delta t$ the central node selects a routing protocol and calculates the routing tables, which are then sent to slave nodes for internal configuration
				      \item then, a feedback is collected by the central node about the performance of the protocol, which may make the central node change the current routing protocol for the next $\Delta t$
			      \end{itemize}

			      After some $\Delta t$, the system converges to the most adequate routing protocol.
		\end{enumerate}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Unicast or Multicast}
	\framesubtitle{Classification criteria: Context of use}

	Categorizing between \tbf{unicast} or \tbf{multicast} approaches is a natural choice, given the inherent \tit{overhead} that multicast routing protocols introduce. \\
	\vspace{0.8cm}

	\pause
	Indeed, RL should be applied in multicasting scenarios only when links are sufficiently stable and/or partial delivery is allowed, otherwise convergence may be outright \tit{impossible}.
\end{frame}

\begin{frame}
	\frametitle{Unicast or Multicast}
	\framesubtitle{Classification criteria: Context of use}

	\begin{colorblock}[sintefdarkgreen]{sinteflightgreen}{Example: a Multicast protocol}
		An example of a multicast protocol is the \tbf{FROMS (\tit{Feedback Routing for Optimizing Multiple Sinks})}, introduced by \textcite{forster}. \\

		\vspace{0.8cm}

		\pause

		This was the first RL-based protocol for multicast routing in WSNs, and it operates as follows:

		\begin{itemize}
			\item it constructs a tree similar to a \tit{Steiner tree} with the selected paths
			\item routing to multiple destinations is defined as the \tbf{minimum cost multicast tree} starting at the source and reaching all interested destinations
			\item the \tit{cost} of the \tit{Steiner-like tree} is defined as the number of one-hop broadcasts to reach all sinks
		\end{itemize}
	\end{colorblock}
\end{frame}

\begin{frame}
	\frametitle{QoS metrics for optimization}
	\framesubtitle{Classification criteria: Context of use}

	The choice of the \tbf{metrics} is one of the most important aspects of a protocol. When multiple metrics are utilized, they are \tit{weighted} based on the importance --- which depends on the target application. \\

	\vspace{0.8cm}

	\pause

	QoS metrics that have been addressed as objectives for RL-based routing include:

	\begin{overprint}
		\onslide<1>
		\onslide<2>
		\begin{itemize}
			\item \tbf{delivery rate}: average time to deliver a packet
			\item \tbf{delivery ratio}: proportion of packets successfully delivered
			\item \tbf{hop count}: average number of hops from source to destination
			\item \tbf{loss ratio}: proportion of packets not delivered
		\end{itemize}

		\onslide<3>

		\begin{itemize}
			\item \tbf{bandwidth}: average bandwidth provided to sources
			\item \tbf{throughput}: average amount of bytes delivered in the entire network per time unit
			\item \tbf{path stability}: change in path between source and destination over time
			\item \tbf{energy consumption}: average energy consumption of the network
		\end{itemize}

		\onslide<4>

		\begin{itemize}
			\item \tbf{network lifetime}: average time over which the network is still alive
			\item \tbf{transmission power}: power for performing a transmission
			\item \tbf{hit delay}: average delay to return requrested data in P2P networks
			\item \tbf{hit ratio}: proportion of satisfied requrests in P2P networks
		\end{itemize}

		\onslide<5>

		\begin{itemize}
			\item \tbf{gain}: average revenue (in \$) received by the agent --- in business contexts
			\item \tbf{overhead}: average \tit{cost} to deliver data packets at destination --- the \tit{cost} definition depends on the application
		\end{itemize}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{QoS metrics for optimization}
	\framesubtitle{Classification criteria: Context of use}

	\begin{colorblock}[sintefdarkgreen]{sinteflightgreen}{Example: a multi-metric protocol}
		The first protocol in the literature that considered \tit{multiple metrics} is the \tbf{AdaR (\tit{Adaptive Routing})}, proposed by \textcite{wang}. \\

		\vspace{0.8cm}

		\pause

		In particular, they considered \tbf{four QoS metrics} for path selections:

		\begin{enumerate}
			\item number of hops
			\item residual energy
			\item link reliability
			\item number of routes crossing in a node
		\end{enumerate}
	\end{colorblock}
\end{frame}

\begin{frame}
	\frametitle{QoS guaranteeing}
	\framesubtitle{Classification criteria: Context of use}

	Lastly, a few routing protocols are aimed at providing \tbf{QoS guarantees}, regarding delivery delay to meet some requirements of \tit{delay-sensitive applications}. \\

	\vspace{0.8cm}

	\pause

	For instance, QoS guarantees are essential in \tit{multimedia applications}, such as video streams and streaming services.
\end{frame}

\begin{frame}
	\frametitle{QoS guaranteeing}
	\framesubtitle{Classification criteria: Context of use}

	\begin{colorblock}[sintefdarkgreen]{sinteflightgreen}{Example: a delay-aware protocol}
		An example of a protocol that provides \tit{soft} delay guarantees to delay-sensitive applications was introduced by \textcite{lin}, called \tbf{RL-RPC (\tit{RL-based Routing and Power Control})}. \\

		\vspace{0.8cm}

		\pause

		In this protocol, RL is used to learn \tbf{channel conditions}. Moreover

		\begin{itemize}
			\item at each node the protocol selects the best route and the best power to forward packets
			\item a packet is dropped when the \tit{deadline} --- included in the packet --- can no more be satisfied
		\end{itemize}
	\end{colorblock}
\end{frame}

\begin{frame}
	\frametitle{Design characteristics}
	\framesubtitle{Classification criteria: Design characteristics}

	The second group examines the \tbf{internal design choices} behind each protocol. \\

	\vspace{0.8cm}

	\pause

	This encompasses how states, actions, rewards, and learning models are defined, as well as the architectural decisions that shape how an agent interacts with the network.
\end{frame}

\begin{frame}
	\frametitle{Learning model}
	\framesubtitle{Classification criteria: Design characteristics}

	In RL there are two possible approaches, \tbf{model-free} and \tbf{model-based} learning. \\

	\vspace{0.8cm}

	\begin{overprint}
		\onslide<1>

		The vast majority of RL-based routing algorithms are \tbf{model-free}, since constructing a model requires knowledge about the environment that can be difficult to collect.

		\onslide<2>

		However a few algorithms are actually \tbf{model-based}, in particular

		\begin{itemize}
			\item some of them use \tit{offline}-collected information of the environment model
			\item some others calculate and improve the environment model in an \tit{online} fashion
		\end{itemize}

		\onslide<3>

		Model based approaches are known to converge quickly, and thus can offer an interesting opportunity when the \tbf{speed of convergence} is a crucial requirement.
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Learning model}
	\framesubtitle{Classification criteria: Design characteristics}

	\begin{colorblock}[sintefdarkgreen]{sinteflightgreen}{Example: a moded-based protocol}
		For instance, the \tbf{QGrid (Q-learning-based Grid routing)} protocol introduced by \textcite{qgrid} is a model-based protocol for VANETs, since it is composed of two phases:

		\begin{itemize}
			\item \tit{offline learning} of Q-values
			\item \tit{online use} of the Q-value table to forward packets
		\end{itemize}

		\vspace{0.8cm}

		\begin{overprint}
			\onslide<1>

			A vehicle uses its offline Q-table to pick the \tit{next grid} and forwards the packet to a neighbor in that new grid; if none exists, it forwards to a closer neighbor, or otherwise stores the packet until a new neighbor appears. \\

			\onslide<2>


			The assumption of this protocol is that it is possible to infer the optimal path to reach some destination $d$ located in a given grid from the \tbf{history of inter-grid movements}.
		\end{overprint}
	\end{colorblock}
\end{frame}

\begin{frame}
	\frametitle{Agent states}
	\framesubtitle{Classification criteria: Design characteristics}

	In order to apply RL to any optimization problem, we need some definition of the \tbf{state space}, the set of all possible states that the agent can be in. \\

	\vspace{0.8cm}

	\pause

	The definitions of state spaces in the reviewed literature include:

	\begin{itemize}
		\item set of \tit{nodes}, the most popular in RL-based routing protocols
		\item set of \tit{grids}, used in grid-organized networks
		\item set of \tit{couples}, relating to the dynamics of the nodes --- for instance in VANETs a \tit{couple} is a vehicle speed class together with its context of move (urban, highway, etc.)
		\item set of \tit{paths} and their characteristics
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Actions spaces}
	\framesubtitle{Classification criteria: Design characteristics}

	Protocols can be also classified based on the \tbf{action space} they define. This table contains the possible \tit{single-type actions} and the corresponding \tit{action spaces}:

	\begin{overprint}
		\onslide<1>

		\begin{table}[H]
			\centering
			\begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
				\hline
				\textbf{Action selection}                                  & \textbf{Action space}         \\
				\hline
				Select node $j$ as next hop and forward packet             & Set of node IDs               \\
				\hline
				Select a subset of neighbors $S$ and broadcast packet      & Set of partitions of node IDs \\
				\hline
				Select output link $l$ and transmit packet                 & Set of links                  \\
				\hline
				Select grid $g$ and send packet to one of the nodes in $g$ & Set of grids                  \\
				\hline
			\end{tabular}
		\end{table}

		\onslide<2>

		\begin{table}[H]
			\centering
			\begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
				\hline
				\textbf{Action selection}                                                                      & \textbf{Action space}            \\
				\hline

				Select predefined path $p$ and send packet along $p$                                           & Set of predefined paths          \\
				\hline
				Allocate $m$ free channels                                                                     & Set of channels                  \\
				\hline
				Select a transmission power $pw$                                                               & Set of transmission power levels \\
				\hline
				Select a protocol $prt$ among a list of routing protocols and configure the network with $prt$ & Set of standard protocols        \\
				\hline
			\end{tabular}
		\end{table}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Solution space exploration}
	\framesubtitle{Classification criteria: Design characteristics}

	In RL the \tbf{Exploration vs Exploitation dilemma} is a well-known problem. Indeed, the \tit{speed of convergence} strictly depends on the approach utilized to balance between \tit{exploring} and \tit{exploiting} the solution space.

	\vspace{0.8cm}

	\pause

	The \tit{action selection} strategies in RL-based routing include:

	\begin{overprint}
		\onslide<1>
		\onslide<2>

		\begin{itemize}
			\item \tbf{Greedy strategy}: only the highest Q-value is used for selection --- this strategy may take a very long time to converge
		\end{itemize}

		\onslide<3>

		\begin{itemize}
			\item \tbf{$\varepsilon$-greedy strategy}: in addition to the greedy strategy, the learner uses a small amount of randomness (that depends on $\varepsilon$) to explore new solutions --- the most used form of selection
		\end{itemize}

		\onslide<4>

		\begin{itemize}
			\item \tbf{Proabability based strategy}: similar to $\varepsilon{}$-greedy, but the value of $\varepsilon$ is calculated from the history of learning
		\end{itemize}

		\onslide<5>

		\begin{itemize}
			\item \tbf{Bayesian network decision strategy}: the action selection uses \tit{Bayesian networks} to better explore the solution space
		\end{itemize}

		\onslide<6>

		\begin{itemize}
			\item \tbf{Devaluation of solutions based strategy}: the Q-values are periodically decayed in order to enforce exploration of the solution space
		\end{itemize}

		\onslide<7>

		\begin{itemize}
			\item \tbf{New neighbors first strategy}: newly discovered nodes are favored in next hop selection --- this approach is particularly useful in \tit{mobile networks}
		\end{itemize}

	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Agents collaboration}
	\framesubtitle{Classification criteria: Design characteristics}

	The original version of RL defines each agent as \tit{independent}, and only able to interact with the environment. \\

	\vspace{0.8cm}

	\begin{overprint}
		\onslide<1>

		However, when applying RL to routing it is more effective to allow \tbf{collaborating agents}, indeed almost all reviewed protocols are based on this idea. \\

		\vspace{0.8cm}

		Note that \tit{collaboration} only concerns \tit{non}-RL related exchanges, such as the exchange of \tbf{link-state information}.

		\onslide<2>

		Indeed, collaboration is so prevalent among the protocols in the literature that it is possible to categorize them w.r.t. how the nodes cooperate:

		\begin{itemize}
			\item \tbf{Reactive collaboration}: nodes only provide feedback upon reception of packet
			\item \tbf{Proactive collaboration}: similar to the \tit{reactive} approach, but nodes additionally broadcast their link-state information through \tit{Hello packets} to their neighbors
		\end{itemize}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Hybridization with other optimization techniques}
	\framesubtitle{Classification criteria: Design characteristics}

	Most of RL-based routing algorithms involve \tit{pure} RL approaches, however some algorithms combine RL with other \tbf{optimization techniques} to speed up convergence. \\

	\vspace{0.8cm}

	\pause

	Hybrid optimization approaches include:

	\begin{overprint}
		\onslide<1>
		\onslide<2>

		\begin{itemize}
			\item Gradient methods
			\item Game Theory approaches
			\item \tit{Bayesian network} methods
			\item Least square policy iteration
		\end{itemize}

		\onslide<3>

		\begin{itemize}
			\item Neural Networks
			\item Genetic algorithms
			\item Ants optimization
		\end{itemize}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Hybridization with other optimization techniques}
	\framesubtitle{Classification criteria: Design characteristics}


	\begin{colorblock}[sintefdarkgreen]{sinteflightgreen}{Example: a hybrid protocol}
		An example of a protocol that combines RL with other optimization techniques is \tbf{AdaR} (the first multi-metric protocol) \cite{wang}. \\

		\vspace{0.8cm}

		\pause

		In particular, AdaR uses \tbf{Least Squares Policy Iteration (LSPI)}, which indeed enables \tit{faster convergence} to optimal solution without suffering initial parameter setting.
	\end{colorblock}
\end{frame}

\begin{frame}
	\frametitle{Numbers of parameters to tune}
	\framesubtitle{Classification criteria: Design characteristics}

	A well-designed protocol should be \tbf{easily tunable}. However, in addition to $\alpha$ and $\gamma$ a multitude of protocols utilize many more tunable parameters in their algorithms. \\

	\vspace{0.8cm}

	\pause

	Additionally, weights must be assigned whenever there are \tbf{multiple metrics} to consider. This may add too much complexity in terms of usability for the correct choice of the parameters. \\

	\vspace{0.8cm}

	\pause

	Therefore the authors categorized the routing protocols also based on the \tbf{number of tunable QoS metrics and parameters} each paper offers.
\end{frame}

\begin{frame}
	\frametitle{Reward functions}
	\framesubtitle{Classification criteria: Design characteristics}

	The authors outline that the \tbf{reward function} is the most distinctive feature of existing RL-based routing protocols. \\

	\pause

	Reward functions may be categorized into 3 classes:

	\begin{overprint}
		\onslide<1>
		\onslide<2>

		\begin{enumerate}
			\item \tbf{Test-based reward functions}: the reward is assigned a constant value, depending the outcome of some \tit{test}. \\

			      \vspace{0.8cm}

			      The most common test is checking if the packet was actually delivered to destination, which yields a \tit{binary outcome} for the reward.
		\end{enumerate}

		\onslide<3>

		\begin{enumerate}
			\setcounter{enumi}{1}
			\item \tbf{Linear reward functions}: they have the following general form $$R = C + \sum_{k = 1}^H {\omega_k \cdot M_k}$$

			      \begin{itemize}
				      \item $C$ is a constant factor that depends on the test chosen by the protocol
				      \item $H$ is the number of metrics of the protocol
				      \item $\omega_k$ is the weight of the $k$-th metric
				      \item $M_k$ is the value of the $k$-th metric
			      \end{itemize}
		\end{enumerate}

		\onslide<4>

		\begin{enumerate}
			\setcounter{enumi}{2}
			\item \tbf{Nonlinear reward functions}: this type is less common among RL-protocols, and they are designed with different forms of combinations of metrics depending on the specific application
		\end{enumerate}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Q-value updating rule forms}
	\framesubtitle{Classification criteria: Design characteristics}

	Over half of proposed RL-based routing algorithms are direct applications of Q-learning as originally proposed by Watkins. \\

	\vspace{0.8cm}

	\pause

	However the remaining half of the protocols use procedures that either

	\begin{itemize}
		\item use a \tit{modified} Q-value updating rule, or
		\item do not rely on Q-learning at all
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Example of a non Q-learning compliant protocol}
	\framesubtitle{Classification criteria: Design characteristics}

	TODO maybe? and maybe remove something previously?
\end{frame}

\begin{frame}
	\frametitle{Performance aspects}
	\framesubtitle{Classification criteria: Performance aspects}

	Lastly, the third group evaluates protocols through their \tbf{performance outcomes}. \\

	\vspace{0.8cm}

	\pause

	This includes qualitative assessments of overhead, responsiveness, and the metrics used to judge routing effectiveness.
\end{frame}

\begin{frame}
	\frametitle{Communication overhead}
	\framesubtitle{Classification criteria: Performance aspects}

	\tbf{Communication overhead} is a crucial part of the design of a routing protocol, which depends on how the protocol defines the exchange of relvant information between nodes of the network. \\

	\vspace{0.8cm}

	\pause

	Therefore, the overhead of the reviewed protocols have been categorized from a \tit{qualitative} point of view into:


	\begin{overprint}
		\onslide<1>
		\onslide<2>

		\begin{itemize}
			\item \tbf{null overhead}: there is no exchange of information between agents
		\end{itemize}

		\onslide<3>

		\begin{itemize}
			\item \tbf{low overhead}: the chosen next hop returns a feedback in an explicit ACK packet, or it includes its feedback when, in turn, it (re)forwards the packet
		\end{itemize}

		\onslide<4>

		\begin{itemize}
			\item \tbf{medium overhead}: this is the case of protocols in which the feedback from the destination is propagated to all hops through an explicit ACK packet
		\end{itemize}

		\onslide<5>

		\begin{itemize}
			\item \tbf{medium overhead}: this is the case of protocols in which the feedback from the destination is propagated to all hops through an explicits ACK packet
		\end{itemize}

		\onslide<6>

		\begin{itemize}
			\item \tbf{high overhead}: these protocols require that nodes periodically exchange link-state information
		\end{itemize}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{State space overhead}
	\framesubtitle{Classification criteria: Performance aspects}

	Even if this aspect is sometimes neglected, RL-based algorithms require \tit{memory} to store the \tbf{states of the agents}, and the number of states may be very high. \\

	\vspace{0.8cm}

	\pause

	Hence, the protocols can be \tit{qualitatively} grouped based on the \tbf{state space overhead}:

	\begin{overprint}
		\onslide<1>

		\onslide<2>

		\begin{itemize}
			\item \tbf{very low overhead}: when state space is the possible states of a packet
		\end{itemize}

		\onslide<3>

		\begin{itemize}
			\item \tbf{low overhead}: when the state space is the node IDs
		\end{itemize}

		\onslide<4>

		\begin{itemize}
			\item \tbf{limited overhead}: when the state space depends on external factors --- e.g. the number of transmission power levels, the maximum number of available channels, etc.
		\end{itemize}

		\onslide<5>

		\begin{itemize}
			\item \tbf{high overhead}: when the state space is a list of whole paths with their current characteristics
		\end{itemize}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Action space overhead}
	\framesubtitle{Classification criteria: Performance aspects}

	Additionally, RL-based algorithms also require \tit{memory} to store all the \tbf{possible actions} that agents can perform. \\

	\vspace{0.8cm}

	\pause

	Therefore again, the protocols can be \tit{qualitatively} grouped based on the \tbf{action space overhead}:

	\begin{overprint}
		\onslide<1>

		\onslide<2>

		\begin{itemize}
			\item \tbf{low overhead}: when the action space depends on external factors TODO WHAT WHY
		\end{itemize}

		\onslide<3>

		\begin{itemize}
			\item \tbf{medium overhead}: when the action space depends on the number of nodes in the neighborhood
		\end{itemize}

		\onslide<4>

		\begin{itemize}
			\item \tbf{high overhead}: when the action space depends on either
			      \begin{itemize}
				      \item the number of \tit{dynamic paths}
				      \item the number of or \tit{predefined paths}
				      \item the number of \tit{grids} in the network
			      \end{itemize}
		\end{itemize}

		\onslide<5>

		\begin{itemize}
			\item \tbf{very high overhead}: when the state space depends on combinations of channels subsets or paths
		\end{itemize}
	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Proof of convergence}
	\framesubtitle{Classification criteria: Performance aspects}

	In the optimization field, the \tbf{convergence} to optimal solutions is an \tit{expected} property. Nevertheless, many existing techniques to solve multicriteria optimization problems are not guaranteed to reach the optimal solution. \\

	\vspace{0.8cm}

	% \pause



	% \pause

	\begin{overprint}
		\onslide<1>
		Regarding RL-based algorithms, from the original work of Watkins it is possible to derive proofs of convergence, however:

		\begin{itemize}
			\item not all RL-based approaches are based on the standard implementation of Q-learning
			\item many papers rely on \tbf{additional assumptions} that \curlyquotes{guarantee} convergence, but in real world scenarios it is hard to establish the \tbf{satisfiability} of such assumptions
		\end{itemize}

		\vspace{0.8cm}

		\onslide<2>

		In general, proving convergence rigorously remains an \tbf{open issue} for most protocols that are not perfectly Q-learning compliant. \\

		\vspace{0.8cm}

		Rather, convergence is usually assessed from \tbf{simulations} or just \tit{stated as reachable eventually} without providing any proof of such claim.

	\end{overprint}
\end{frame}

\begin{frame}
	\frametitle{Protocol performance (in simulations)}
	\framesubtitle{Classification criteria: Performance aspects}

	It would be clearly important to categorize the protocols based on the \tbf{performance} achieved in simulations. However, given the

	\begin{itemize}
		\item large number of reviewed protocols
		\item wide variety of reward functions
		\item wide range of metric weights
	\end{itemize}

	the authors found impratical to even provide \tit{qualitative} evaluation of the performance of each single protocol. \\

	\vspace{0.8cm}

	\pause

	Therefore, they limited their effort to present the data reported by the original authors \tit{as-is}, without introducing any qualitative consideration.
\end{frame}

\section{Conclusion and challenges}

\begin{frame}{Conclusion}
	RL is an efficient alternative to design routing protocols that

	\begin{itemize}
		\item provide higher level of QoS
		\item optimize resource utilization
	\end{itemize}

	\vspace{0.8cm}

	\pause

	However, some \tbf{challenges} still remain and should be investigated further to provide evidence on applicability of RL-based protocols \tit{at large scale}.
\end{frame}

\begin{frame}{Proof of optimality}
	As already mentioned, almost all reviewed papers \tit{did not} convincingly address \tbf{proof of convergence}. \\

	\vspace{0.8cm}

	\pause

	Since convergence is such an important requirement in the context of optimization, it should be treated as a core property of any RL-based routing approach. \\

	\vspace{0.8cm}

	\pause

	Indeed, without it there is no guarantee that the learned policy will stabilize or consistently produce optimal/reliable routing decisions.
\end{frame}

\begin{frame}{Speed of convergence}
	Whenever large networks are considered, \tit{space exploration} may take a very long time before optimal paths are discovered. This may result in poor end-to-end performance of the network. \\
	\vspace{0.8cm}

	\pause

	\tbf{Convergence rates} should be investigated to provide \tbf{bounds} of delay for let users know when the network can or cannot provide \tit{acceptable QoS levels}.
\end{frame}

\begin{frame}{Link-state information dissemination}
	In most protocols, \tbf{link-state information} is used to calculate metrics, hence the convergence of the routing algorithms strictly depends on the \tit{freshness} of disseminated information. \\

	\vspace{0.8cm}

	\pause

	Therefore, the frequence of \tit{Hello packets} should be addressed in order to find a compromise between \tbf{protocol overhead} and \tbf{values of reward}.
\end{frame}

\begin{frame}{Metrics weights and learning parameters}
	The \tbf{learning parameters} and the weights of the metrics have a significant impact on

	\begin{itemize}
		\item the \tit{quality} of paths
		\item the \tit{speed} of convergence
	\end{itemize}

	However, determining the optimal set of parameters can be very \tit{difficult}. \\

	\vspace{0.8cm}

	\pause

	Hence, the authors suggest the development of a \tbf{methodology} to address this problem, which would make the deployment of RL-based routing protocols easier.
\end{frame}

\begin{frame}{Hybridization}
	A few routing protocols have sufficiently reduced the search space by \tbf{hybridizing} the optimal-path search with external optimization techniques. \\

	\vspace{0.8cm}

	\pause

	However, the authors underline that this strategy is not explored enough. RL should be used \tit{jointly} with \tbf{other techniques} to provide more (soft) guarantees on exploration of the solution space.

	\vspace{0.8cm}

	\pause

	In recent years \tbf{Deep Learning (DL)} has been proposed to enable RL to scale to complex problems.
\end{frame}

\begin{frame}{Predicting traffic demands}
	Learning in current RL-based protocols is mainly based on \tbf{network-oriented metrics} like

	\begin{itemize}
		\item delays
		\item loss rate
		\item transmission success
		\item mobility of nodes
	\end{itemize}

	\vspace{0.8cm}

	\pause

	However, \tbf{predicting traffic} from sources to destinations would result in more efficient selection of forwarders. Indeed, in \tit{supervised learning scenarios} traffic predicion has resulted in more efficient route selection.
\end{frame}

\begin{frame}{Cooperative learning}
	Lastly, almost all proposed protocols are \tbf{independent-agent-based}. In fact even if the agents \curlyquotes{collaborate} by exchanging link-state information, they don't \tit{learn cooperatively}. \\

	\vspace{0.8cm}

	\pause

	To face complexity of future networks, the authors suggest to enable \tbf{collaboration} between agents to help design more robust and efficient learning approaches.
\end{frame}

\begin{frame}
	\centering
	{\fontsize{40}{48}\selectfont \textbf{Thanks for your attention}}
\end{frame}

\end{document}
